{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import clip\n",
    "\n",
    "# main_df = pd.read_csv('/home/mayowa/Documents/IU/indiana_reports.csv')  \n",
    "\n",
    "# id_df = pd.read_csv('/home/mayowa/Documents/IU/indiana_projections.csv')  \n",
    "\n",
    "# main_df.loc[:5,[\"impression\"]]\n",
    "\n",
    "# # Check if column 'impression\"' has missing values\n",
    "# has_missing_values = main_df[\"impression\"].isna().any()\n",
    "# print(f\"Column impression has missing values: {has_missing_values}\")\n",
    "\n",
    "# print(main_df[\"impression\"].isna().sum()) \n",
    "\n",
    "# clean_df = main_df.dropna(subset=[\"impression\"])\n",
    "\n",
    "# has_missing_values = clean_df[\"impression\"].isna().any()\n",
    "# print(f\"Column impression has missing values: {has_missing_values}\")\n",
    "\n",
    "# print(clean_df[\"impression\"].isna().sum()) \n",
    "\n",
    "# file_path = '/home/mayowa/Documents/cleaned_reports.csv'\n",
    "\n",
    "# clean_df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(main_df[\"uid\"][0])\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, image_csv_path, text_csv_path):\n",
    "        # Load CSV files\n",
    "        self.image_df = pd.read_csv(image_csv_path)\n",
    "        self.text_df = pd.read_csv(text_csv_path)\n",
    "        \n",
    "        # both CSVs have a \"uid\" column for linking\n",
    "        self.image_path = self.image_df['filename'].tolist()\n",
    "        self.uid = self.image_df['uid'].tolist()\n",
    "        \n",
    "        # Initialize an empty list to store text data\n",
    "        self.text_data = []\n",
    "\n",
    "        # Match \"uid\" values and load \"impression\" column from text CSV\n",
    "        for uid in self.uid:\n",
    "            text_row = self.text_df.loc[self.text_df['uid'] == uid]\n",
    "            if not text_row.empty:\n",
    "                self.text_data.append(text_row.iloc[0]['image'])\n",
    "            else:\n",
    "                # Handle cases where there is no matching \"uid\"\n",
    "                self.text_data.append(\"\")  # or any other suitable placeholder\n",
    "        # Tokenize text\n",
    "        self.title = clip.tokenize(self.text_data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = preprocess(Image.open(f\"/home/Documents/IU/images/images_normalized/{self.image_path[idx]}\"))\n",
    "#         #print(image)\n",
    "#         #title = clip.tokenize(self.title[idx])\n",
    "#         #print(title)\n",
    "#         return image, self.title\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(f\"/home/mayowa/Documents/IU/images/images_normalized/{self.image_path[idx]}\"))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "\n",
    "# Example usage:\n",
    "# image_csv_path = 'image_data.csv'\n",
    "# text_csv_path = 'text_data.csv'\n",
    "# dataset = ImageTitleDataset(image_csv_path, text_csv_path)\n",
    "# image, title = dataset[0]\n",
    "\n",
    "\n",
    "# Example usage: \n",
    "image_csv_path = '/home/mayowa/Documents/IU/indiana_projections.csv'\n",
    "text_csv_path = '/home/mayowa/Documents/IU/indiana_reports.csv'\n",
    "dataset = ImageTitleDataset(image_csv_path, text_csv_path)\n",
    "\n",
    "dataset[0][1]\n",
    "\n",
    "dataset[0][0]\n",
    "\n",
    "len(dataset.text_data)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE, shuffle=True) #Define your own dataloader\n",
    "\n",
    "imagei, texti = next(iter(train_dataloader))\n",
    "\n",
    "imagei, texti\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "  for batch in tqdm(train_dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      images,texts = batch \n",
    "    \n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "    \n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "      print(total_loss)\n",
    "\n",
    "\n",
    "# torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': total_loss,\n",
    "#         }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename\n",
    "\n",
    "# model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "# checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# #checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# #checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# #checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
