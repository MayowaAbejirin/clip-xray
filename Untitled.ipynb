{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645664bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/934 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m images\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    137\u001b[0m texts \u001b[38;5;241m=\u001b[39m texts\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 139\u001b[0m logits_per_image, logits_per_text \u001b[38;5;241m=\u001b[39m model(images, texts)\n\u001b[1;32m    141\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(images),dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    143\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m (loss_img(logits_per_image,ground_truth) \u001b[38;5;241m+\u001b[39m loss_txt(logits_per_text,ground_truth))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m/mnt/2bd0c619-af65-4059-bb97-0110ea804e3b/mubuntum/pytorchenv/pytorchenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/2bd0c619-af65-4059-bb97-0110ea804e3b/mubuntum/pytorchenv/pytorchenv/lib/python3.11/site-packages/clip/model.py:359\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, text):\n\u001b[0;32m--> 359\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_image(image)\n\u001b[1;32m    360\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_text(text)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# normalized features\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/2bd0c619-af65-4059-bb97-0110ea804e3b/mubuntum/pytorchenv/pytorchenv/lib/python3.11/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual(image\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import clip\n",
    "\n",
    "# main_df = pd.read_csv('/home/mayowa/Documents/IU/indiana_reports.csv')  \n",
    "\n",
    "# id_df = pd.read_csv('/home/mayowa/Documents/IU/indiana_projections.csv')  \n",
    "\n",
    "# main_df.loc[:5,[\"impression\"]]\n",
    "\n",
    "# # Check if column 'impression\"' has missing values\n",
    "# has_missing_values = main_df[\"impression\"].isna().any()\n",
    "# print(f\"Column impression has missing values: {has_missing_values}\")\n",
    "\n",
    "# print(main_df[\"impression\"].isna().sum()) \n",
    "\n",
    "# clean_df = main_df.dropna(subset=[\"impression\"])\n",
    "\n",
    "# has_missing_values = clean_df[\"impression\"].isna().any()\n",
    "# print(f\"Column impression has missing values: {has_missing_values}\")\n",
    "\n",
    "# print(clean_df[\"impression\"].isna().sum()) \n",
    "\n",
    "# file_path = '/home/mayowa/Documents/cleaned_reports.csv'\n",
    "\n",
    "# clean_df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(main_df[\"uid\"][0])\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "\n",
    "\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, image_csv_path, text_csv_path):\n",
    "        # Load CSV files\n",
    "        self.image_df = pd.read_csv(image_csv_path)\n",
    "        self.text_df = pd.read_csv(text_csv_path)\n",
    "        \n",
    "        # both CSVs have a \"uid\" column for linking\n",
    "        self.image_path = self.image_df['filename'].tolist()\n",
    "        self.uid = self.image_df['uid'].tolist()\n",
    "        \n",
    "        # Initialize an empty list to store text data\n",
    "        self.text_data = []\n",
    "\n",
    "        # Match \"uid\" values and load \"impression\" column from text CSV\n",
    "        for uid in self.uid:\n",
    "            text_row = self.text_df.loc[self.text_df['uid'] == uid]\n",
    "            if not text_row.empty:\n",
    "                self.text_data.append(text_row.iloc[0]['image'])\n",
    "            else:\n",
    "                # Handle cases where there is no matching \"uid\"\n",
    "                self.text_data.append(\"\")  # or any other suitable placeholder\n",
    "        # Tokenize text\n",
    "        self.title = clip.tokenize(self.text_data)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = preprocess(Image.open(f\"/home/Documents/IU/images/images_normalized/{self.image_path[idx]}\"))\n",
    "#         #print(image)\n",
    "#         #title = clip.tokenize(self.title[idx])\n",
    "#         #print(title)\n",
    "#         return image, self.title\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(f\"/home/mayowa/Documents/IU/images/images_normalized/{self.image_path[idx]}\"))\n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "\n",
    "# Example usage:\n",
    "# image_csv_path = 'image_data.csv'\n",
    "# text_csv_path = 'text_data.csv'\n",
    "# dataset = ImageTitleDataset(image_csv_path, text_csv_path)\n",
    "# image, title = dataset[0]\n",
    "\n",
    "\n",
    "# Example usage: \n",
    "image_csv_path = '/home/mayowa/Documents/IU/indiana_projections.csv'\n",
    "text_csv_path = '/home/mayowa/Documents/IU/indiana_reports.csv'\n",
    "dataset = ImageTitleDataset(image_csv_path, text_csv_path)\n",
    "\n",
    "dataset[0][1]\n",
    "\n",
    "dataset[0][0]\n",
    "\n",
    "len(dataset.text_data)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE, shuffle=True) #Define your own dataloader\n",
    "\n",
    "imagei, texti = next(iter(train_dataloader))\n",
    "\n",
    "imagei, texti\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()\n",
    "else :\n",
    "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "  for batch in tqdm(train_dataloader):\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      images,texts = batch \n",
    "    \n",
    "      images= images.to(device)\n",
    "      texts = texts.to(device)\n",
    "    \n",
    "      logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "      total_loss.backward()\n",
    "      if device == \"cpu\":\n",
    "         optimizer.step()\n",
    "      else : \n",
    "        convert_models_to_fp32(model)\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "      print(total_loss)\n",
    "\n",
    "\n",
    "# torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': total_loss,\n",
    "#         }, f\"model_checkpoint/model_10.pt\") #just change to your preferred folder/filename\n",
    "\n",
    "# model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "# checkpoint = torch.load(\"model_checkpoint/model_10.pt\")\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# #checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# #checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# #checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361274a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
